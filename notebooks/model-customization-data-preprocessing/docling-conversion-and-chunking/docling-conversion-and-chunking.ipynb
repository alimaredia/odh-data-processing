{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c83150f-fa8b-42a1-8974-ce9483912fba",
   "metadata": {},
   "source": [
    "# Document Conversion with Docling\n",
    "\n",
    "This notebook uses [Docling](https://github.com/docling-project/docling) to convert any type of document into a Docling Document: a structured representation of the original document that can be exported as JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf4340c-cfd4-418c-955b-be8c0d544e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq docling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5e2659-e626-4235-97fc-f311adf8f5b7",
   "metadata": {},
   "source": [
    "### Set directory for files to convert and output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d12166-8b40-4c46-9147-27cfc1c8b09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "sample_data_dir = Path(\"data/sample-pdfs\")\n",
    "files = list((sample_data_dir.glob(\"*.pdf\")))\n",
    "\n",
    "output_dir = Path(\"data/output\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d4e3ef-2fdb-45a3-a217-b07638a35363",
   "metadata": {},
   "source": [
    "### Configure Docling conversion pipeline\n",
    "\n",
    "Next we set the configuration options for our conversion pipeline. \n",
    "\n",
    "The standard pipeline options generally yield good and fast results for most documents. In some cases, however, alternative conversion conversion pipelines can lead to better outcomes. For instance, OCR is effective for scanned documents or images that contain text to be extracted and analyzed. In cases where other conversion pipelines didn't produce good results, using a vision-language model (VLM) may be a good option.\n",
    "\n",
    "The next cell contains three combinations of pipeline options: the default (standard) options, a variant that forces OCR on the entire document, and another that uses a VLM. You can comment or uncomment the corresponding code blocks to switch between them or create a custom combination of settings. For more information and additional conversion conversion pipelines, check our [Docling Conversion Tutorials](https://github.com/instructlab/examples/blob/main/docs/docling-conversion/README.md).\n",
    "\n",
    "For a complete reference on Docling's conversion pipeline configuration, check the [Examples](https://docling-project.github.io/docling/examples/) section of the official documentation, as well as the [PDFPipelineOptions](https://docling-project.github.io/docling/reference/pipeline_options/#docling.datamodel.pipeline_options.PdfPipelineOptions) and [PDFFormatOptions](https://docling-project.github.io/docling/reference/document_converter/#docling.document_converter.InputFormat.XML_JATS) reference pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be47eb2-8e2d-445c-a5de-fcdd17ef7097",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.datamodel.accelerator_options import AcceleratorDevice, AcceleratorOptions\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import (\n",
    "    EasyOcrOptions,\n",
    "    PdfPipelineOptions,\n",
    "    VlmPipelineOptions,\n",
    "    smoldocling_vlm_conversion_options,\n",
    ")\n",
    "from docling.pipeline.vlm_pipeline import VlmPipeline\n",
    "from docling.backend.docling_parse_v4_backend import DoclingParseV4DocumentBackend\n",
    "\n",
    "# Standard pipeline options\n",
    "pipeline_options = PdfPipelineOptions()\n",
    "doc_converter = DocumentConverter(\n",
    "    format_options={\n",
    "        InputFormat.PDF: PdfFormatOption(\n",
    "            pipeline_options=pipeline_options\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "# Force OCR on the entire page\n",
    "# pipeline_options = PdfPipelineOptions()\n",
    "# pipeline_options.do_ocr = True\n",
    "# pipeline_options.ocr_options.force_full_page_ocr = True\n",
    "# pipeline_options.ocr_options.lang = [\"en\"]\n",
    "# pipeline_options.ocr_options = EasyOcrOptions(force_full_page_ocr=True)\n",
    "# pipeline_options.accelerator_options = AcceleratorOptions(\n",
    "#     num_threads=4, device=AcceleratorDevice.AUTO\n",
    "# )\n",
    "# doc_converter = DocumentConverter(\n",
    "#     format_options={\n",
    "#         InputFormat.PDF: PdfFormatOption(\n",
    "#             pipeline_options=pipeline_options,\n",
    "#             backend=DoclingParseV4DocumentBackend,\n",
    "#         )\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# Use the SmolDocling VLM\n",
    "# pipeline_options = VlmPipelineOptions()\n",
    "# pipeline_options.vlm_options = smoldocling_vlm_conversion_options\n",
    "# doc_converter = DocumentConverter(\n",
    "#     format_options={\n",
    "#         InputFormat.PDF: PdfFormatOption(\n",
    "#             pipeline_options=pipeline_options,\n",
    "#             pipeline_cls=VlmPipeline,\n",
    "#         )\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ab035e-e05e-41e8-be90-23527b5d4bc4",
   "metadata": {},
   "source": [
    "Finally, we convert every document into Docling JSON as long as it is a [valid file type](https://docling-project.github.io/docling/usage/supported_formats/) to be converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a45e16-8fa0-4223-9890-7d75f6869aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "confidence_reports = dict()\n",
    "\n",
    "json_files=[]\n",
    "             \n",
    "for file in files:\n",
    "    conversion_result = doc_converter.convert(source=file)\n",
    "\n",
    "    doc = conversion_result.document\n",
    "    doc_dict = doc.export_to_dict()\n",
    "    confidence_reports[file] = conversion_result.confidence\n",
    "\n",
    "\n",
    "    json_output_path = output_dir / f\"{file.stem}.json\"\n",
    "    with open(json_output_path, \"w\") as f:\n",
    "        json.dump(doc_dict, f)\n",
    "        print(f\"Path of JSON output is: {Path(json_output_path).resolve()}\")\n",
    "        json_files.append(json_output_path.resolve())\n",
    "\n",
    "    print(\"Document sample:\\n\")\n",
    "    print(f\"{doc.export_to_text()[:500]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13802403-9342-4e33-96a9-b04b4fedd070",
   "metadata": {},
   "source": [
    "### EXPERIMENTAL: Conversion confidence\n",
    "\n",
    "When converting a document, Docling can calculate how confident it is in the quality of the conversion. This *confidence* is expressed as both a *score* and a *grade*. The score is a numeric value between 0 and 1, and the grade is a label that can be **poor**, **fair**, **good**, or **excellent**. If Docling is unable to calculate a confidence grade, the value will be marked as *unspecified*.\n",
    "\n",
    "If your document receives a low score (for example, below 0.8) and a grade of *poor* or *fair*, you'll probably benefit from using a different conversion technique. In that case, go back to the *Configure Docling Conversion Pipeline* section and try selecting a different approach (e.g. forcing OCR or using a VLM) and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32624c0e-f32c-48c9-85ba-6ab6a3d4cebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file, confidence_report in confidence_reports.items():\n",
    "    print(f\"Conversion confidence for {file}:\")\n",
    "    \n",
    "    print(f\"Average confidence: \\x1b[1m{confidence_report.mean_grade.name}\\033[0m (score {confidence_report.mean_score:.3f})\")\n",
    "    \n",
    "    low_score_pages = []\n",
    "    for page in confidence_report.pages:\n",
    "        page_confidence_report = confidence_report.pages[page]\n",
    "        if page_confidence_report.mean_score < confidence_report.mean_score:\n",
    "            low_score_pages.append(page)\n",
    "\n",
    "    print(f\"Pages that scored lower than average: {', '.join(str(x + 1) for x in low_score_pages)}\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15efd01a-b964-4959-9099-2020b0f5c7f6",
   "metadata": {},
   "source": [
    "# Chunking with Docling\n",
    "\n",
    "The goal of chunking converted documents is to break them down into smaller and logical pieces.\n",
    "\n",
    "Docling has its own built in [chunking](https://docling-project.github.io/docling/examples/hybrid_chunking/#hybrid-chunking) that will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a6dda1-234b-4dae-9d3c-9ee54d8d9df8",
   "metadata": {},
   "source": [
    "### Initialize the Chunker\n",
    "\n",
    "Docling provides two chunkers, the `HierarchicalChunker` and the `HybridChunker`.\n",
    "The `HierarchicalChunker` creates chunks based on the hierarchy in the Docling document\n",
    "\n",
    "The `HybridChunker` builds on the `HierarchicalChunker` and by making it tokenization aware.\n",
    "\n",
    "The `HybridChunker` has options for a `tokenizer`, the `max_tokens` in a chunk, and `merge_peers` to merge undersized chunks that are next to eachother. Uncomment the commented out code to configure these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af69ab48-ae81-4de1-9445-047913490c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from docling_core.transforms.chunker.tokenizer.huggingface import HuggingFaceTokenizer\n",
    "#from transformers import AutoTokenizer\n",
    "\n",
    "from docling.chunking import HybridChunker\n",
    "\n",
    "#EMBED_MODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "#MAX_TOKENS = 1024\n",
    "#\n",
    "# tokenizer = HuggingFaceTokenizer(\n",
    "#     tokenizer=AutoTokenizer.from_pretrained(EMBED_MODEL_ID),\n",
    "#     max_tokens=MAX_TOKENS,  # optional, by default derived from `tokenizer` for HF case\n",
    "#     merge_peers=True # \n",
    "# )\n",
    "\n",
    "chunker = HybridChunker(\n",
    "    #tokenizer=tokenizer,\n",
    "    #merge_peers=True,  # whether to merge undersized chunks - defaults to True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a714b4-6851-458c-a0b1-c28205c6ffaa",
   "metadata": {},
   "source": [
    "### Load and chunk the converted docling document\n",
    "\n",
    "Next lets convert the document we want to chunk up into a Docling Document.\n",
    "\n",
    "The resulting chunks are stored in a file called chunks.jsonl in the `chunks` directory in your contribution. This file is used as an input in a later step when creating the seed dataset for SDG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c65151-3023-4971-bbb4-3a6557d35ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "all_chunks = []\n",
    "    \n",
    "for file in json_files:\n",
    "    # reconvert the docling JSON for chunking\n",
    "    doc = DocumentConverter().convert(source=file)\n",
    "\n",
    "    document_chunks = []\n",
    "    chunk_iter = chunker.chunk(dl_doc=doc.document)\n",
    "    chunk_objs = list(chunk_iter)\n",
    "\n",
    "    print(f\"Extracted {len(chunk_objs)} chunks from {doc.document.name}\")\n",
    "    \n",
    "    for chunk in chunk_objs:\n",
    "        c = dict(chunk=chunker.contextualize(chunk=chunk), file=doc.document.name,metadata=chunk.meta.export_json_dict())\n",
    "        document_chunks.append(c)\n",
    "        all_chunks.append(c)\n",
    "\n",
    "    document_chunk_dir = output_dir / f\"{doc.document.name}\"\n",
    "    document_chunk_dir.mkdir(parents=True, exist_ok=True)\n",
    "    chunks_file_path = document_chunk_dir / \"chunks.jsonl\"\n",
    "    with open(chunks_file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        for chunk in document_chunks:\n",
    "            json.dump(chunk, file)\n",
    "            file.write(\"\\n\")\n",
    "        print(f\"Path of chunks JSON is: {Path(chunks_file_path).resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22512fd8-b9f9-4452-b54b-af2c281d6ede",
   "metadata": {},
   "source": [
    "### View the Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9b7dd8-87e6-43ea-9eeb-392502c45190",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_gen = iter(all_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bad47a-e1f6-4007-b7f8-fa24d53966a8",
   "metadata": {},
   "source": [
    "The document is now broken into small sections with metadata about the chunk based on the document's format. To view the chunks one by one, rerun the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2702903-2a94-49c7-aed7-8ffbf91408ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(chunk_gen)['chunk'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda56aaa-ff52-4358-a209-b2b1d1b93337",
   "metadata": {},
   "source": [
    "To view several randomly selected chunks, run the following cell as many times as you like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0175cd25-c43d-41d8-8212-f31c41954391",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CHUNKS_TO_VIEW = 5\n",
    "\n",
    "import random\n",
    "import json\n",
    "\n",
    "sample = random.sample(all_chunks, min(len(all_chunks), NUM_CHUNKS_TO_VIEW))\n",
    "\n",
    "i = 1\n",
    "for chunk in sample:\n",
    "    print(f\"== Randomly selected chunk {i}: ==========\\n\\n{chunk['chunk']}\\n\\n\")\n",
    "    i += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
